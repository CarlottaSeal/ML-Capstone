Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           WTI-log_TimeMixer_fold3Model:              TimeMixer           

[1mData Loader[0m
  Data:               custom              Root Path:          ./results_WTI_WalkForward_TimeMixer_20251128_055449/fold_3/
  Data Path:          WTI_fold3.csv       Features:           MS                  
  Target:             daily_return        Freq:               d                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            512                 Label Len:          0                   
  Pred Len:           6                   Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             6                   Dec In:             6                   
  C Out:              1                   d model:            16                  
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               32                  
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        0                   Itr:                1                   
  Train Epochs:       100                 Batch Size:         64                  
  Patience:           15                  Learning Rate:      0.001               
  Des:                WalkForward_Fold3   Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_WTI-log_TimeMixer_fold3_TimeMixer_custom_ftMS_sl512_ll0_pl6_dm16_nh8_el2_dl1_df32_expand2_dc4_fc1_ebtimeF_dtTrue_WalkForward_Fold3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 6962
val 1064
test 2132
	iters: 100, epoch: 1 | loss: 1.9020919
	speed: 0.0145s/iter; left time: 156.2971s
Epoch: 1 cost time: 1.539644718170166
Epoch: 1, Steps: 109 | Train Loss: 1.1375583 Vali Loss: 0.9176480 Test Loss: 1.3192912
Validation loss decreased (inf --> 0.917648).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 1.0065007
	speed: 0.0128s/iter; left time: 137.0081s
Epoch: 2 cost time: 1.0429296493530273
Epoch: 2, Steps: 109 | Train Loss: 1.0824766 Vali Loss: 0.8977526 Test Loss: 1.3029119
Validation loss decreased (0.917648 --> 0.897753).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.9962654
	speed: 0.0122s/iter; left time: 128.7591s
Epoch: 3 cost time: 0.9437029361724854
Epoch: 3, Steps: 109 | Train Loss: 1.0317933 Vali Loss: 0.9040256 Test Loss: 1.3313709
EarlyStopping counter: 1 out of 15
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 1.0543033
	speed: 0.0108s/iter; left time: 112.6533s
Epoch: 4 cost time: 0.9412765502929688
Epoch: 4, Steps: 109 | Train Loss: 0.9802782 Vali Loss: 0.9323445 Test Loss: 1.3718022
EarlyStopping counter: 2 out of 15
Updating learning rate to 0.000125
	iters: 100, epoch: 5 | loss: 0.9299161
	speed: 0.0111s/iter; left time: 115.1171s
Epoch: 5 cost time: 0.962592601776123
Epoch: 5, Steps: 109 | Train Loss: 0.9325043 Vali Loss: 0.9711839 Test Loss: 1.4200281
EarlyStopping counter: 3 out of 15
Updating learning rate to 6.25e-05
	iters: 100, epoch: 6 | loss: 0.6097288
	speed: 0.0116s/iter; left time: 119.4577s
Epoch: 6 cost time: 1.0213544368743896
Epoch: 6, Steps: 109 | Train Loss: 0.8957042 Vali Loss: 0.9688624 Test Loss: 1.4501823
EarlyStopping counter: 4 out of 15
Updating learning rate to 3.125e-05
	iters: 100, epoch: 7 | loss: 0.9695444
	speed: 0.0108s/iter; left time: 109.8692s
Epoch: 7 cost time: 0.9394886493682861
Epoch: 7, Steps: 109 | Train Loss: 0.8703657 Vali Loss: 0.9704610 Test Loss: 1.4520230
EarlyStopping counter: 5 out of 15
Updating learning rate to 1.5625e-05
	iters: 100, epoch: 8 | loss: 1.2226003
	speed: 0.0122s/iter; left time: 122.8290s
Epoch: 8 cost time: 1.1022253036499023
Epoch: 8, Steps: 109 | Train Loss: 0.8584854 Vali Loss: 0.9745978 Test Loss: 1.4550084
EarlyStopping counter: 6 out of 15
Updating learning rate to 7.8125e-06
	iters: 100, epoch: 9 | loss: 0.8749211
	speed: 0.0163s/iter; left time: 161.4096s
Epoch: 9 cost time: 1.5058887004852295
Epoch: 9, Steps: 109 | Train Loss: 0.8519383 Vali Loss: 0.9779904 Test Loss: 1.4583565
EarlyStopping counter: 7 out of 15
Updating learning rate to 3.90625e-06
	iters: 100, epoch: 10 | loss: 0.9468213
	speed: 0.0112s/iter; left time: 110.0168s
Epoch: 10 cost time: 0.9333798885345459
Epoch: 10, Steps: 109 | Train Loss: 0.8471958 Vali Loss: 0.9810008 Test Loss: 1.4594873
EarlyStopping counter: 8 out of 15
Updating learning rate to 1.953125e-06
	iters: 100, epoch: 11 | loss: 1.4075334
	speed: 0.0106s/iter; left time: 102.8109s
Epoch: 11 cost time: 0.918734073638916
Epoch: 11, Steps: 109 | Train Loss: 0.8465913 Vali Loss: 0.9805693 Test Loss: 1.4607359
EarlyStopping counter: 9 out of 15
Updating learning rate to 9.765625e-07
	iters: 100, epoch: 12 | loss: 1.0944717
	speed: 0.0106s/iter; left time: 101.6255s
Epoch: 12 cost time: 0.9182519912719727
Epoch: 12, Steps: 109 | Train Loss: 0.8440519 Vali Loss: 0.9754090 Test Loss: 1.4609350
EarlyStopping counter: 10 out of 15
Updating learning rate to 4.8828125e-07
	iters: 100, epoch: 13 | loss: 0.7735219
	speed: 0.0107s/iter; left time: 101.1439s
Epoch: 13 cost time: 0.9258716106414795
Epoch: 13, Steps: 109 | Train Loss: 0.8436207 Vali Loss: 0.9833969 Test Loss: 1.4611889
EarlyStopping counter: 11 out of 15
Updating learning rate to 2.44140625e-07
	iters: 100, epoch: 14 | loss: 0.8149064
	speed: 0.0106s/iter; left time: 99.8030s
Epoch: 14 cost time: 0.9221348762512207
Epoch: 14, Steps: 109 | Train Loss: 0.8453069 Vali Loss: 0.9885373 Test Loss: 1.4612792
EarlyStopping counter: 12 out of 15
Updating learning rate to 1.220703125e-07
	iters: 100, epoch: 15 | loss: 0.6913285
	speed: 0.0106s/iter; left time: 98.3882s
Epoch: 15 cost time: 0.9211664199829102
Epoch: 15, Steps: 109 | Train Loss: 0.8449880 Vali Loss: 0.9788216 Test Loss: 1.4613440
EarlyStopping counter: 13 out of 15
Updating learning rate to 6.103515625e-08
	iters: 100, epoch: 16 | loss: 0.8966858
	speed: 0.0111s/iter; left time: 102.0009s
Epoch: 16 cost time: 0.9966082572937012
Epoch: 16, Steps: 109 | Train Loss: 0.8423340 Vali Loss: 0.9836496 Test Loss: 1.4613723
EarlyStopping counter: 14 out of 15
Updating learning rate to 3.0517578125e-08
	iters: 100, epoch: 17 | loss: 1.1612532
	speed: 0.0122s/iter; left time: 110.2068s
Epoch: 17 cost time: 1.05299711227417
Epoch: 17, Steps: 109 | Train Loss: 0.8435365 Vali Loss: 0.9738865 Test Loss: 1.4613823
EarlyStopping counter: 15 out of 15
Early stopping
>>>>>>>testing : long_term_forecast_WTI-log_TimeMixer_fold3_TimeMixer_custom_ftMS_sl512_ll0_pl6_dm16_nh8_el2_dl1_df32_expand2_dc4_fc1_ebtimeF_dtTrue_WalkForward_Fold3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2132
test shape: (2132, 6, 1) (2132, 6, 1)
test shape: (2132, 6, 1) (2132, 6, 1)
mse:1.3242, mae:0.6924, msIC:0.0163, msIR:0.0373
Total Evaluation 

MSE:1.3242Â±0.0000
MAE:0.6924Â±0.0000
msIC:0.0163Â±0.0000
msIR:0.0373Â±0.0000
